{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 Function-based RL\n",
    "Ponwalai Chalermwattanatrai 65340500042"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each algorithm, describe whether it follows a value-based, policy-based, or Actor-Critic approach, specify the type of policy it learns (stochastic or deterministic), identify the type of observation space and action space (discrete or continuous), and explain how each advanced RL method balances exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Linear Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Approach**: Value-based\n",
    "- **Policy type**: Deterministic (Stochastic during training)\n",
    "    - Always chooses the action with the maximum Q-value (argmax Q(s, a)) -> Deterministic\n",
    "    - During training, using epsilon-greedy, which includes some random -> Stochastic\n",
    "- **Observation space**: continuous\n",
    "- **Action space**: discrete\n",
    "- **Balances exploration and exploitation**: using epsilon-greedy\n",
    "    - Selects a random action with probability epsilon and decay epsilon over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept of algorithm**\n",
    "\n",
    "Linear Q-learning is a Q-learning algorithm that use function approximation instead of a Q-table. It stores a weight matrix, where each column corresponds to the weights for a particular action. The size of the weight matrix is [state_size x num_actions].\n",
    "\n",
    "In Linear- Q learning Q-value is linear function calculated using the dot product between the state vector and the weight vector for the selected action:\n",
    "\n",
    "$$Q(s, a) = \\mathbf{w}_a^\\top \\mathbf{s}$$\n",
    "\n",
    "After get action and apply to environment, to update the weights, we calculate the Temporal Difference (TD) error using the Bellman equation:\n",
    "\n",
    "$$\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$$\n",
    "\n",
    "and define loss function as the mean squared error of TD error (1/2 is Constant that add for easier differential equation which is not impact to value):\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{2} \\delta^2$$\n",
    "\n",
    "To minimize this loss, we apply gradient descent. The gradient of the loss with respect to the weights is:\n",
    "\n",
    "$$\\nabla_{\\mathbf{w}_a} \\mathcal{L} = -\\delta \\cdot \\mathbf{s}$$\n",
    "\n",
    "Therefore, the weights are updated using the following rule:\n",
    "\n",
    "$$\\mathbf{w}_a \\leftarrow \\mathbf{w}_a + \\alpha \\cdot \\delta \\cdot \\mathbf{s}$$\n",
    "\n",
    "Which can be written as:\n",
    "\n",
    "$$\\boxed{\n",
    "\\mathbf{w}_a^{\\text{new}} = \\mathbf{w}_a^{\\text{old}} + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right) \\cdot \\mathbf{s}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Psuedo code of training Linear Q learning:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Initialize weight vector w[a] for each action a\n",
    "Set epsilon to initial value\n",
    "\n",
    "For each episode:\n",
    "    Initialize environment and get initial state\n",
    "    state = initial_state\n",
    "    done = false\n",
    "\n",
    "    While not done:\n",
    "        # epsilon-greedy action selection\n",
    "        Generate a random number r ∈ [0, 1]\n",
    "        if r < epsilon:\n",
    "            action = random action\n",
    "        else:\n",
    "            for each action a:\n",
    "                Q[a] = dot(state, w[a])\n",
    "            action = action with max Q[a]\n",
    "\n",
    "        Take action in the environment\n",
    "        Observe next_state, reward, done\n",
    "\n",
    "        # TD Update\n",
    "        current_Q = dot(state, w[action])\n",
    "        For each action a:\n",
    "            next_Q[a] = dot(next_state, w[a])\n",
    "        max_next_Q = max over next_Q[a]\n",
    "\n",
    "        td_target = reward + discount_factor * max_next_Q\n",
    "        td_error = td_target - current_Q\n",
    "\n",
    "        # Gradient descent update\n",
    "        w[action] = w[action] + learning_rate * td_error * state\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    Decay epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. DQN (Deep Q-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Approach**: Value-based\n",
    "- **Policy type**: Deterministic (Stochastic during training)\n",
    "    - Always chooses the action with the maximum Q-value (argmax Q(s, a)) -> Deterministic\n",
    "    - During training, using epsilon-greedy, which includes some random -> Stochastic\n",
    "- **Observation space**: continuous\n",
    "- **Action space**: discrete\n",
    "- **Balances exploration and exploitation**: using epsilon-greedy\n",
    "    - Selects a random action with probability epsilon and decay epsilon over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept of algorithm**\n",
    "\n",
    "DQN (Deep Q-Network) is conceptually similar to Linear Q-learning, but instead of using a linear function for function approximation, DQN uses a neural network to approximate the Q-value function.\n",
    "\n",
    "DQN has 3 main components:\n",
    "1. replay memory\n",
    "    - A buffer used to store the agent’s past experiences as tuples:(state, action, reward, next state)\n",
    "    - Using for traning, during training it randomly sample mini-batches of experiences from the buffer to update policy net.\n",
    "    - mini-batches will breaks the temporal correlation between consecutive experiences and improves stability and sample efficiency.\n",
    "\n",
    "2. Policy net\n",
    "    - A neural network that approximates the Q-value.\n",
    "    - Similar to the weight vector in Linear Q-learning but with hidden layers for greater function approximation.\n",
    "    - The network is updated using TD error and gradient descent:\n",
    "    $$\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$$\n",
    "    - The loss function is the smooth L1 loss (Huber loss):\n",
    "    $$\n",
    "    \\mathcal{L} =\n",
    "    \\begin{cases}\n",
    "    \\frac{1}{2} \\delta^2 & \\text{if } |\\delta| < 1 \\\\\n",
    "    |\\delta| - \\frac{1}{2} & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    - This behaves like MSE for small TD errors and like MAE for large errors — making it less sensitive to outliers.\n",
    "\n",
    "3. Target net\n",
    "    - A copy of the policy network that is used to calculate the Q-value for the next state.\n",
    "    - It prevents the TD target from shifting too quickly, which can destabilize training.\n",
    "    - Rather than updating the target network every step, DQN uses a soft update mechanism.\n",
    "    $$\\theta_{\\text{target}} \\leftarrow \\tau \\cdot \\theta_{\\text{policy}} + (1 - \\tau) \\cdot \\theta_{\\text{target}}$$\n",
    "    Where:\n",
    "    - $\\theta_{\\text{policy}}$​: weights of the policy network\n",
    "    - $\\theta_{\\text{target}}$​: weights of the target network\n",
    "    - $\\tau$: soft update rate (between [0,1])\n",
    "    \n",
    "    Soft update is prevents rapid oscillations in learning\n",
    "    \n",
    "**The relationship of 3 component is**\n",
    "\n",
    "![image3.png](img/image3.png)\n",
    "\n",
    "source: https://www.theengineeringprojects.com/2024/01/deep-q-networks-dqn-reinforcement-learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Psuedo code of training DQN:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Initialize replay_memory with size = buffer_size\n",
    "    - Stores (state, action, reward, next_state, done) tuples\n",
    "    - Follows FIFO replacement when full\n",
    "\n",
    "Initialize policy_net: a neural network\n",
    "    - Input size = state dimension\n",
    "    - Output size = number of actions\n",
    "\n",
    "Initialize target_net: a copy of policy_net\n",
    "    - Same architecture as policy_net\n",
    "    - Used to compute stable TD targets\n",
    "\n",
    "Set epsilon to initial value\n",
    "\n",
    "for each episode:\n",
    "    Initialize environment and get initial state\n",
    "    state = initial_state\n",
    "    done = false\n",
    "\n",
    "    While not done:\n",
    "        # epsilon-greedy action selection\n",
    "        Generate a random number r ∈ [0, 1]\n",
    "        if r < epsilon:\n",
    "            action = random action\n",
    "        else:\n",
    "            Q_values = policy_net(state) # Q_values size = number of actions\n",
    "            action = argmax(Q_values)\n",
    "\n",
    "        Take action in the environment\n",
    "        Observe next_state, reward, done\n",
    "        Store (state, action, reward, next_state, done) in replay_memory\n",
    "\n",
    "        # Sample mini-batch from replay_memory\n",
    "        batch = sample(batch_size) from replay_memory\n",
    "        Initialize loss_list = []\n",
    "\n",
    "        #update policy_net\n",
    "        for (s, a, r, s', is_done) in batch:\n",
    "            current_Q = policy_net(s)[a]\n",
    "            if is_done:\n",
    "                td_target = r\n",
    "            else:\n",
    "                next_Q = max(target_net(s'))\n",
    "                td_target = r + discount_factor * next_Q\n",
    "            loss = smooth_l1_loss(current_Q, td_target)\n",
    "            add loss to loss_list\n",
    "        mean_loss = average(loss_list)\n",
    "        Perform gradient descent on policy_net using mean_loss\n",
    "\n",
    "        #Soft update target_net\n",
    "        for each parameter key in policy_net:\n",
    "            target_net[key] = tau * policy_net[key] + (1 - tau) * target_net[key]\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    Decay epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. MC REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Approach**: Policy-based\n",
    "- **Policy type**: Stochastic\n",
    "    - Learns a probability distribution over actions (Categorical distribution)\n",
    "- **Observation space**: continuous\n",
    "- **Action space**: discrete\n",
    "- **Balances exploration and exploitation**: using stochastic policy\n",
    "    - In MC REINFORCE action sample from probability distribution, this means the agent naturally explores, since each action has some non-zero probability of being selected.\n",
    "    - Exploit start when policy increases the probability of high-reward actions through policy gradient updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept of algorithm**\n",
    "\n",
    "MC REINFORCE is a Monte Carlo method which using policy gradient to learn stochastic policy. It using complete return of the entire episode from current policy to update new policy.\n",
    "\n",
    "MC REINFORCE directly models the policy using a neural network (policy net) and optimizes it to maximize returns.\n",
    "\n",
    "Action Selection:\n",
    "- MC REINFORCE is policy-based, meaning it directly models the policy $\\pi(a∣s)$ as a neural network (policy net).\n",
    "- Given a state s, the policy net outputs a probability distribution over actions.\n",
    "- A Categorical distribution is constructed from these probabilities, and an action is sampled from it\n",
    "\n",
    "Policy Update\n",
    "- In policy update we want to maximize the expected return:\n",
    "$$J(\\theta) = \\mathbb{E} [ \\sum_{t=0}^{T} r_t ]$$\n",
    "- Using the episodic policy gradient theorem, we get a formula for how to change the policy parameters $\\theta$ to increase $J(\\theta)$:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E} [\\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot G_t ]$$\n",
    "- Since most optimizers minimize a loss function, we define the loss as the negative of the policy gradient:\n",
    "$$\\mathcal{L} = -\\nabla_\\theta J(\\theta) = - \\sum_{t=0}^{T} \\log \\pi_\\theta(a_t \\mid s_t) \\cdot G_t$$\n",
    "- So, minimizing the loss is equivalent to maximizing the expected return.\n",
    "\n",
    "from equation, to update the policy, we need:\n",
    "1. Returns $G_t$​ at each timestep:\n",
    "    - Calculate from the total discounted future rewards from time t: \n",
    "    $$G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... = r_t + \\gamma G_{t+1}​$$\n",
    "2. Log-probabilities of selected actions:\n",
    "    - For each timestep, we store:\n",
    "    $$log\\pi_{\\theta}(a_t​∣s_t​)$$\n",
    "    - This term gives us a score function gradient, which tells us how the log-probability of the action changes with respect to the policy parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Psuedo code of training MC REINFORCE:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Initialize policy_net: a neural network\n",
    "    - Input size = state dimension\n",
    "    - Output size = number of actions (via softmax)\n",
    "\n",
    "for each episode:\n",
    "    Initialize environment and get initial state\n",
    "    state = initial_state\n",
    "    done = false\n",
    "\n",
    "    Initialize reward_list = []\n",
    "    Initialize log_probs_list = []\n",
    "\n",
    "    while not done:\n",
    "        # select action from policy\n",
    "        probability = policy_net(state)\n",
    "        distribution = Categorical(probability)\n",
    "        action = sample from distribution\n",
    "\n",
    "        Take action in the environment\n",
    "        Observe next_state, reward, done\n",
    "        add reward to reward_list\n",
    "        add log(distribution[action]) to log_probs_list\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    #calculate stepwise returns\n",
    "    Initialize return_list = []\n",
    "    for r in reverse(reward):\n",
    "        G = r + discount_factor * G\n",
    "        add G to return_list\n",
    "\n",
    "    #update policy\n",
    "    loss = 0\n",
    "    for each (r, log_prob) in (return_list, log_probs_list)\n",
    "        loss = loss - (log_prob * r)\n",
    "    loss = loss / len(return_list)\n",
    "    \n",
    "    Perform gradient descent on policy_net using loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. A2C (Advantage Actor–Critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Approach**: Actor-Critic\n",
    "- **Policy type**: Stochastic\n",
    "- **Observation space**: continuous\n",
    "- **Action space**: discrete / continuous (in this project we do discrete)\n",
    "- **Balances exploration and exploitation**: \n",
    "    - using stochastic policy\n",
    "        - In MC REINFORCE action sample from probability distribution, this means the agent naturally explores, since each action has some non-zero probability of being selected.\n",
    "        - Exploit start when policy increases the probability of high-reward actions through policy gradient updates\n",
    "    - using Entropy\n",
    "        - Entropy measures the randomness of the policy.\n",
    "        - If all actions have similar probabilities(explore) entropy is high and if strongly prefers one action (exploit), entropy is low.\n",
    "        - Entropy loss is added (as a negative bonus) to the actor loss so it will make agent confident in that action.\n",
    "        - A high entropy coefficient makes the agent explore more (slows down exploitation), while a low coefficient lets the agent exploit more aggressively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept of Algorithm**\n",
    "\n",
    "A2C (Advantage Actor-Critic) is an actor-critic reinforcement learning algorithm that uses the advantage function to compute the actor’s loss and improve learning efficiency.\n",
    "- The actor network selects actions based on the current policy. and updated using the advantage estimate.\n",
    "- The critic network estimates the value of states to using for compute the advantage. and updated using the temporal difference (TD) error.\n",
    "\n",
    "Component of A2C\n",
    "- Actor-network: neural network that takes the state as input and outputs a probability distribution over actions (policy).\n",
    "    - Update Actor network by using Advantage\n",
    "    $$A = Q(s,a) - V(s)$$\n",
    "    Where:\n",
    "    - $Q(s,a)$: Expected return from taking action a in state s\n",
    "    - $V(s)$: Expected return from state s following the policy\n",
    "    - $A(s,a)$: Extra benefit of taking action a instead of the average action\n",
    "\n",
    "    In A2C, advantage have estimate as:\n",
    "    $$A_t = r + \\gamma V(s') - V(s)$$\n",
    "\n",
    "    Then use it to calculate actor loss:\n",
    "    $$L_{\\text{actor}} = -\\log \\pi(a_t \\mid s_t) \\cdot A_t$$\n",
    "\n",
    "    - Entropy Bonus\n",
    "        - An entropy term is added to the actor loss to encourage exploration.\n",
    "        - Entropy is calculated from the action probabilities:\n",
    "            - If actions have similar probabilities: Entropy will high (more exploration)\n",
    "            - If actions have strongly prefers one action: Entropy will low (more exploitation)\n",
    "    The total actor loss becomes:\n",
    "    $$L_{\\text{actor}} = -\\log \\pi(a_t \\mid s_t) \\cdot A_t - entropy_coef * entropy$$\n",
    "    \n",
    "    Including entropy in the loss helps the agent explore by keeping its action choices more varied early in training. Over time, it naturally becomes more confident as it learns which actions are better.\n",
    "    - This loss is minimized using gradient descent to improve the value prediction accuracy.\n",
    "\n",
    "- Critic-network: A neural network that estimates the value function V(s) (the expected return from a given state).\n",
    "    - Update critic net by using TD-error:\n",
    "    $$\\delta = r + \\gamma V(s') - V(s)$$\n",
    "\n",
    "    - Loss function is mean square error of TD error:\n",
    "    $$L_{\\text{critic}} =\\frac{1}{2} \\delta^2$$\n",
    "\n",
    "    - This loss is minimized using gradient descent to improve the value prediction accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Psuedo code of training A2C:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Initialize replay_memory with size = buffer_size\n",
    "    - Stores (state, action, reward, next_state, done) tuples\n",
    "    - Follows FIFO replacement when full\n",
    "\n",
    "Initialize actor_net: a neural network which calculate policy\n",
    "    - Input size = state dimension\n",
    "    - Output size = number of actions\n",
    "\n",
    "Initialize critic_net: a neural network which calculate value\n",
    "    - Input size = state dimension\n",
    "    - Output size = 1 (value of that state)\n",
    "\n",
    "for each episode:\n",
    "    Initialize environment and get initial state\n",
    "    state = initial_state\n",
    "    done = false\n",
    "\n",
    "    While not done:\n",
    "        # select action from actor_net\n",
    "        probability = actor_net(state)\n",
    "        distribution = Categorical(probability)\n",
    "        action = sample from distribution\n",
    "\n",
    "        Take action in the environment\n",
    "        Observe next_state, reward, done\n",
    "        Store (state, action, reward, next_state, done) in replay_memory\n",
    "\n",
    "        # Sample mini-batch from replay_memory\n",
    "        batch = sample(batch_size) from replay_memory\n",
    "        Initialize critic_loss_list, policy_loss_list = [], []\n",
    "\n",
    "        #calculate loss and update neural network\n",
    "        for (s, a, r, s', is_done) in batch:\n",
    "            value = critic_net(s)\n",
    "            next_values = self.critic(s')\n",
    "            td_target = r + ~is_done * discount_factor * next_values\n",
    "            critic_loss = mse_loss(value, target)\n",
    "\n",
    "            advantage = (td_target - value)\n",
    "            probability = actor_net(state)\n",
    "            distribution = Categorical(probability)\n",
    "            log_probs = log(distribution[action])\n",
    "            entropy = entropy of distribution\n",
    "            actor_loss = (-log_probs * advantage - entropy_coef * entropy)\n",
    "\n",
    "            add critic_loss to critic_loss_list\n",
    "            add policy_loss to policy_loss_list\n",
    "\n",
    "        mean_critic_loss = average(critic_loss_list)\n",
    "        mean_policy_loss = average(policy_loss_list)\n",
    "\n",
    "        Perform gradient descent on critic_net using mean_critic_loss\n",
    "        Perform gradient descent on policy_net using mean_policy_loss\n",
    "\n",
    "        state = next_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Setting up Cart-Pole Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RL base class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Replay Buffer Class**\n",
    "\n",
    "This class using for collect experiences into memory which have size = buffer size and if more than this will handle by FIFO (first in first out) and sampling it for training. It's have 3 sub function\n",
    "- add(state, action_idx, reward, next_state, done): add experience into memory.\n",
    "- sample(): return random experinces size = batch size for use in training.\n",
    "- __len__(): return number of experiences in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, batch_size = 1):\n",
    "        \"\"\"\n",
    "        Initializes the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            buffer_size (int): Maximum number of experiences the buffer can hold.\n",
    "            batch_size (int): Number of experiences to sample per batch.\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def add(self, state, action_idx, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        Adds an experience to the replay buffer.\n",
    "\n",
    "        Args:\n",
    "            state (Tensor): The current state of the environment.\n",
    "            action_idx (int): The action index of action taken at this state.\n",
    "            reward (float): The reward received after taking the action.\n",
    "            next_state (Tensor): The next state resulting from the action.\n",
    "            done (bool): Whether the episode has terminated.\n",
    "        \"\"\"\n",
    "        self.memory.append((state, action_idx, reward, next_state, done))\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Samples a batch of experiences from the replay buffer.\n",
    "\n",
    "        Returns:\n",
    "            tuple:\n",
    "                - state_batch (Tensor): Batch of states.\n",
    "                - action_batch (Tensor): Batch of actions.\n",
    "                - reward_batch (Tensor): Batch of rewards.\n",
    "                - next_state_batch (Tensor): Batch of next states.\n",
    "                - done_batch (Tensor): Batch of terminal state flags.\n",
    "        \"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        state_batch, action_batch, reward_batch, next_state_batch, done_batch = zip(*experiences)\n",
    "\n",
    "        return (\n",
    "            torch.stack(state_batch).to(device),\n",
    "            torch.tensor(action_batch, dtype=torch.long).to(device),\n",
    "            torch.tensor(reward_batch, dtype=torch.float).to(device),\n",
    "            torch.stack(next_state_batch).to(device),\n",
    "            torch.tensor(done_batch, dtype=torch.bool).to(device),\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns the current size of the replay buffer.\n",
    "\n",
    "        Returns:\n",
    "            int: The number of stored experiences.\n",
    "        \"\"\"\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Base Algorithm**\n",
    "\n",
    "This class is a base for others algorithm which is \n",
    "- initialize some important variables such as learning_rate, discout_factor, initial_epsilon, epsilon_decay, final_epsilon, num_of_action, action_range\n",
    "- create a experience replay buffer memory\n",
    "- have 3 sub function:\n",
    "    - scale_action(action_index):\n",
    "        Maps a discrete action in range [0, n] to range [action_min, action_max].\n",
    "    - decay_epsilon\n",
    "        Decay epsilon value to reduce exploration over time. (using in Linear Q-learning and DQN)\n",
    "    - extract_policy_state(obs):\n",
    "        Extract policy state from dict to numpy array then clip and normalize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scale_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_action(self, action):\n",
    "    \"\"\"\n",
    "    Maps a discrete action in range [0, n] to a continuous value in [action_min, action_max].\n",
    "\n",
    "    Args:\n",
    "        action (int): Discrete action in range [0, n].\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Scaled action tensor.\n",
    "    \"\"\"\n",
    "    min_action, max_action = self.action_range\n",
    "    action_step = (max_action - min_action) / (self.num_of_action - 1)\n",
    "    action_value = min_action + action * action_step\n",
    "\n",
    "    return torch.tensor([[action_value]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "decay_epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decay_epsilon(self):\n",
    "    \"\"\"\n",
    "    Decay epsilon value to reduce exploration over time.\n",
    "    \"\"\"\n",
    "    self.epsilon = max(self.final_epsilon, self.epsilon * self.epsilon_decay)\n",
    "    return self.epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extract_policy_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_policy_state(self, obs):\n",
    "    \"\"\"\n",
    "    Extract policy state from dict to numpy array and normalize.\n",
    "\n",
    "    Args:\n",
    "        obs (dict): State observation.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Normalized policy state.\n",
    "    \"\"\"\n",
    "    policy = obs['policy']\n",
    "    state = np.array(policy[:, :4].tolist(), dtype=np.float32)\n",
    "    \n",
    "    # Define bounds as arrays\n",
    "    bound = np.array([ 3,  np.deg2rad(24),  5,  5], dtype=np.float32)\n",
    "    \n",
    "    # Clip to bounds\n",
    "    state = np.clip(state, -1*bound, bound)\n",
    "    \n",
    "    return state / bound"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Q-learning code is write following pseudo code in part1 and split to 6 function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "q(state, action:optional): estimates the Q-value for a given state and (optionally) action by dot product between state and weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q(self, state, a=None):\n",
    "    \"\"\"\n",
    "    Linearly estimates the Q-value for a given state and (optionally) action.\n",
    "\n",
    "    Args:\n",
    "        state (np.array): The current state observation, containing feature representations.\n",
    "        a (int, optional): Action index. If None, returns Q-values for all actions.\n",
    "\n",
    "    Returns:\n",
    "        float or np.array: Q(s, a) if action is specified; otherwise, Q(s, :) for all actions.\n",
    "    \"\"\"\n",
    "    if a==None:\n",
    "        # Get q values from all action in state\n",
    "        return state @ self.w\n",
    "    else:\n",
    "        # Get q values given action & state\n",
    "        return state @ self.w[:, a]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select_action(state): Select an action from Q value or random based on an epsilon-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(self, state):\n",
    "    \"\"\"\n",
    "    Select an action based on an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        state (np.array): The current state of the environment.\n",
    "    \n",
    "    Returns:\n",
    "        tuple (int, Tensor):\n",
    "            - int: Index of the selected action.\n",
    "            - Tensor: The selected action.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < self.epsilon:\n",
    "        action_index = np.random.randint(self.num_of_action)\n",
    "    else:\n",
    "        q_values = self.q(state)\n",
    "        action_index = int(np.argmax(q_values))\n",
    "\n",
    "    return action_index, self.scale_action(action_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update(state, action_idx, reward, next_state, terminated): Updates the weight vector using the Temporal Difference (TD) error and Gradient descent update."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update(\n",
    "    self,\n",
    "    state,\n",
    "    action_idx: int,\n",
    "    reward: float,\n",
    "    next_state,\n",
    "    terminated: bool\n",
    "):\n",
    "    \"\"\"\n",
    "    Updates the weight vector using the Temporal Difference (TD) error \n",
    "    in Q-learning with linear function approximation.\n",
    "\n",
    "    Args:\n",
    "        state (np.array): The current state observation, containing feature representations.\n",
    "        action_idx (int): The action index of action taken in the current state.\n",
    "        reward (float): The reward received for taking the action.\n",
    "        next_state (np.array): The next state observation.\n",
    "        terminated (bool): Whether the episode has ended.\n",
    "\n",
    "    Returns:\n",
    "        float: Temporal Difference (TD) error\n",
    "    \"\"\"\n",
    "    q_current = self.q(state, action_idx)\n",
    "    q_next = np.max(self.q(next_state))\n",
    "    td_target = reward + (self.discount_factor * q_next)\n",
    "    td_error = td_target - q_current\n",
    "\n",
    "    # Gradient descent update\n",
    "    self.w[:, action_idx] += self.lr * td_error * state\n",
    "\n",
    "    return td_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn(env): This is the main of code. In this function we train the agent for 1 episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, env):\n",
    "    \"\"\"\n",
    "    Train the agent for 1 episode. (can using with multi environments)\n",
    "\n",
    "    Args:\n",
    "        env: The environment in which the agent interacts.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[float], List[int], List[float]]:\n",
    "            - List[float]: Episode return for each environment.\n",
    "            - List[int]: Alive time steps for each environment.\n",
    "            - List[float]: Average TD error for each environment.\n",
    "    \"\"\"\n",
    "    obs_list, _ = env.reset()\n",
    "    state_list = self.extract_policy_state(obs_list)\n",
    "    num_envs = len(state_list)\n",
    "    dones = [False] * num_envs\n",
    "    cumulative_rewards = [0.0] * num_envs\n",
    "    steps = [0] * num_envs\n",
    "    losses = [[] for _ in range(num_envs)]\n",
    "\n",
    "    while not all(dones):\n",
    "        # agent stepping\n",
    "        actions_idx = []\n",
    "        actions = []\n",
    "\n",
    "        for i, state in enumerate(state_list):\n",
    "            if dones[i]:\n",
    "                actions_idx.append(0)\n",
    "                actions.append(torch.tensor([[0.0]], dtype=torch.float32))\n",
    "            else:\n",
    "                a_idx, a_cont = self.select_action(state)\n",
    "                actions_idx.append(a_idx)\n",
    "                actions.append(a_cont)\n",
    "        actions = torch.cat(actions, dim=0)\n",
    "\n",
    "        # env stepping\n",
    "        next_obs_list, rewards, terminations, truncations, _ = env.step(actions)\n",
    "        next_state_list = self.extract_policy_state(next_obs_list)\n",
    "        \n",
    "        for i in range(num_envs):\n",
    "            if not dones[i]:\n",
    "                done = bool(terminations[i].item()) or bool(truncations[i].item())\n",
    "                loss = self.update(state_list[i], actions_idx[i], rewards[i].item(), next_state_list[i], done)\n",
    "                losses[i].append(loss)\n",
    "                cumulative_rewards[i] += rewards[i].item()\n",
    "                steps[i] += 1\n",
    "                dones[i] = done\n",
    "                state_list[i] = next_state_list[i]\n",
    "        \n",
    "    self.decay_epsilon()\n",
    "    avg_losses = [np.mean(l) if l else 0.0 for l in losses]\n",
    "\n",
    "    return cumulative_rewards, steps, avg_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_model(path, filename), load_model(path, filename): save and load weight from input path and filename which is save as json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(self, path, filename):\n",
    "    \"\"\"\n",
    "    Save weight parameters.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory to save model.\n",
    "        filename (str): Name of the file.\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    full_path = os.path.join(path, filename)\n",
    "    with open(full_path, 'w') as f:\n",
    "        json.dump(self.w.tolist(), f)\n",
    "        \n",
    "def load_model(self, path, filename):\n",
    "    \"\"\"\n",
    "    Load weight parameters.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory to save model.\n",
    "        filename (str): Name of the file.\n",
    "    \"\"\"\n",
    "    full_path = os.path.join(path, filename)\n",
    "    with open(full_path, 'r') as f:\n",
    "        self.w = np.array(json.load(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN code is write following pseudo code in part1 and split to 8 function. And also have DQN_network class which is class for policy_net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN_network: Neural network model for the Deep Q-Network algorithm for calculate expected Q-value for each state and action Q(s,a)\n",
    "- In DQN_network neural network is init as Linear -> ReLU -> Dropout -> Linear\n",
    "- Have function forward to pass data through the network\n",
    "- In DQN using this neural network for 2 network which is\n",
    "    - policy_net: to estimate Q-value for action selection.\n",
    "    - target_net: soft update from policy_net using for update policy_net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN_network(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network model for the Deep Q-Network algorithm.\n",
    "    \n",
    "    Args:\n",
    "        n_observations (int): Number of input features.\n",
    "        hidden_size (int): Number of hidden neurons.\n",
    "        n_actions (int): Number of possible actions.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_observations, hidden_size, n_actions, dropout):\n",
    "        super(DQN_network, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_observations, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, n_actions)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input state tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Q-value estimates for each action.\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select_action(state): Select an action from Q value or random based on an epsilon-greedy policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(self, state):\n",
    "    \"\"\"\n",
    "    Select an action based on an epsilon-greedy policy.\n",
    "    \n",
    "    Args:\n",
    "        state (np.array): The current state of the environment.\n",
    "    \n",
    "    Returns:\n",
    "        int: action index\n",
    "        Tensor: The selected action.\n",
    "    \"\"\"\n",
    "    if np.random.rand() < self.epsilon:\n",
    "        action_idx = np.random.randint(self.num_of_action)\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            q_values = self.policy_net(state)\n",
    "            action_idx = q_values.argmax(1).item()\n",
    "\n",
    "    return action_idx, self.scale_action(action_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate_loss(non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch): Computes the loss for policy optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(self, non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch):\n",
    "    \"\"\"\n",
    "    Computes the loss for policy optimization.\n",
    "\n",
    "    Args:\n",
    "        non_final_mask (Tensor): Mask indicating which states are non-final.\n",
    "        non_final_next_states (Tensor): The next states that are not terminal.\n",
    "        state_batch (Tensor): Batch of current states.\n",
    "        action_batch (Tensor): Batch of actions taken.\n",
    "        reward_batch (Tensor): Batch of received rewards.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Computed loss.\n",
    "    \"\"\"\n",
    "    state_action_values = self.policy_net(state_batch).gather(1, action_batch.unsqueeze(1)) # shape: [batch_size, 1]\n",
    "    next_state_values = torch.zeros(self.batch_size , device=self.device) # shape: [batch_size]\n",
    "\n",
    "    if non_final_next_states.size(0) > 0:\n",
    "        next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0].detach() # shape: [num_non_final]\n",
    "        \n",
    "    expected_state_action_values = (reward_batch + (self.discount_factor * next_state_values)).unsqueeze(1)\n",
    "    return F.smooth_l1_loss(state_action_values, expected_state_action_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_sample(batch_size): Generates a batch sample from memory(Replay Buffer) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(self, batch_size):\n",
    "    \"\"\"\n",
    "    Generates a batch sample from memory for training.\n",
    "\n",
    "    Returns:\n",
    "        Tuple: A tuple containing:\n",
    "            - non_final_mask (Tensor): A boolean mask indicating which states are non-final.\n",
    "            - non_final_next_states (Tensor): The next states that are not terminal.\n",
    "            - state_batch (Tensor): The batch of current states.\n",
    "            - action_batch (Tensor): The batch of actions taken.\n",
    "            - reward_batch (Tensor): The batch of rewards received.\n",
    "    \"\"\"\n",
    "    if len(self.memory) < batch_size:\n",
    "        return None\n",
    "    states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "    non_final_mask = ~dones\n",
    "    non_final_next_states = next_states[non_final_mask]\n",
    "    return non_final_mask, non_final_next_states, states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update_policy(): Update the policy using the calculated loss from calculate loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(self):\n",
    "    \"\"\"\n",
    "    Update the policy using the calculated loss.\n",
    "\n",
    "    Returns:\n",
    "        float: Loss value after the update.\n",
    "    \"\"\"\n",
    "    # Generate a sample batch\n",
    "    if self.memory.__len__() < self.batch_size:\n",
    "        return\n",
    "    sample = self.generate_sample(self.batch_size)\n",
    "    if sample is None:\n",
    "        return\n",
    "    non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch = sample\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = self.calculate_loss(non_final_mask, non_final_next_states, state_batch, action_batch, reward_batch)\n",
    "\n",
    "    # Perform gradient descent step\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update_target_networks(): Soft update of target network weights using Polyak averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target_networks(self):\n",
    "    \"\"\"\n",
    "    Soft update of target network weights using Polyak averaging.\n",
    "    \"\"\"\n",
    "    # Retrieve the state dictionaries (weights) of both networks\n",
    "    target_net_state_dict = self.target_net.state_dict()\n",
    "    policy_net_state_dict = self.policy_net.state_dict()\n",
    "    \n",
    "    # Apply the soft update rule to each parameter in the target network\n",
    "    for key in target_net_state_dict:\n",
    "        target_net_state_dict[key] = self.tau * policy_net_state_dict[key] + (1.0 - self.tau) * target_net_state_dict[key]\n",
    "    \n",
    "    # Load the updated weights into the target network\n",
    "    self.target_net.load_state_dict(target_net_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn(env): This is the main of code. In this function we train the agent for 1 episode. Statrt with reset enironment and loop for playing action then sampling experience for update policy_net and target_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, env):\n",
    "    \"\"\"\n",
    "    Train the agent for 1 episode. (can using with multi environments)\n",
    "\n",
    "    Args:\n",
    "        env: The environment to train in.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[List[float], List[int], List[float]]:\n",
    "            - List[float]: Episode return for each environment.\n",
    "            - List[int]: Alive time steps for each environment.\n",
    "            - List[float]: Average TD error for each environment.\n",
    "    \"\"\"\n",
    "    obs_list, _ = env.reset()\n",
    "    state_list = self.extract_policy_state(obs_list)\n",
    "    num_envs = len(state_list)\n",
    "    dones = [False] * num_envs\n",
    "    cumulative_rewards = [0.0] * num_envs\n",
    "    steps = [0] * num_envs\n",
    "    loss = [0.0] * num_envs\n",
    "\n",
    "    while not all(dones):\n",
    "        # Predict action from the policy network\n",
    "        actions_idx = []\n",
    "        actions = []\n",
    "\n",
    "        for i, state in enumerate(state_list):\n",
    "            if dones[i]:\n",
    "                actions_idx.append(0)\n",
    "                actions.append(torch.tensor([[0.0]], dtype=torch.float32))\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                a_idx, a_cont = self.select_action(state_tensor)\n",
    "                actions_idx.append(a_idx)\n",
    "                actions.append(a_cont)\n",
    "        actions = torch.cat(actions, dim=0)\n",
    "\n",
    "        # Execute action in the environment and observe next state and reward\n",
    "        next_obs_list, rewards, terminations, truncations, _ = env.step(actions)\n",
    "        next_state_list = self.extract_policy_state(next_obs_list)\n",
    "\n",
    "        # Store the transition in memory\n",
    "        for i in range(num_envs):\n",
    "            if not dones[i]:\n",
    "                done = bool(terminations[i].item()) or bool(truncations[i].item())\n",
    "                self.memory.add(\n",
    "                    torch.tensor(state_list[i], dtype=torch.float32),\n",
    "                    actions_idx[i],\n",
    "                    rewards[i].item(),\n",
    "                    torch.tensor(next_state_list[i], dtype=torch.float32),\n",
    "                    done\n",
    "                )\n",
    "                cumulative_rewards[i] += rewards[i].item()\n",
    "                steps[i] += 1\n",
    "                dones[i] = done\n",
    "                state_list[i] = next_state_list[i]\n",
    "\n",
    "        # Perform one step of the optimization (on the policy network)\n",
    "        loss = self.update_policy()\n",
    "        # Soft update of the target network's weights\n",
    "        self.update_target_networks()\n",
    "\n",
    "    self.decay_epsilon()\n",
    "\n",
    "    return cumulative_rewards, steps, loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_model(path, filename), load_model(path, filename): save and load network model from input path and filename which is save as tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(self, path, filename):\n",
    "    \"\"\"\n",
    "    Save model network.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory to save model.\n",
    "        filename (str): Name of the file.\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    full_path = os.path.join(path, filename)\n",
    "    torch.save({\n",
    "        'policy_net': self.policy_net.state_dict(),\n",
    "        'target_net': self.target_net.state_dict(),\n",
    "        'optimizer': self.optimizer.state_dict(),\n",
    "    }, full_path)\n",
    "\n",
    "def load_model(self, path, filename):\n",
    "    \"\"\"\n",
    "    Load model network.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory to save model.\n",
    "        filename (str): Name of the file.\n",
    "    \"\"\"\n",
    "    full_path = os.path.join(path, filename)\n",
    "    checkpoint = torch.load(full_path, map_location=self.device)\n",
    "    self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "    self.target_net.load_state_dict(checkpoint['target_net'])\n",
    "    self.optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MC REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC REINFORCE code is write following pseudo code in part1 and split to 8 function. And also have MC_REINFORCE_network class which is class for policy_net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC_REINFORCE network: Neural network model for the MC_REINFORCE algorithm. for calculate probability of selecting each action (policy) for each state.\n",
    "- In MC_REINFORCE_network neural network is init as Linear -> ReLU -> Dropout -> Linear -> Softmax\n",
    "- Have function forward to pass data through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_REINFORCE_network(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for the MC_REINFORCE algorithm.\n",
    "    \n",
    "    Args:\n",
    "        n_observations (int): Number of input features.\n",
    "        hidden_size (int): Number of hidden neurons.\n",
    "        n_actions (int): Number of possible actions.\n",
    "        dropout (float): Dropout rate for regularization.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_observations, hidden_size, n_actions, dropout):\n",
    "        super(MC_REINFORCE_network, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(n_observations, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, n_actions),\n",
    "            nn.Softmax(dim=-1)  # Output probabilities\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Args:\n",
    "            x (Tensor): Input tensor.\n",
    "        \n",
    "        Returns:\n",
    "            Tensor: Output tensor representing action probabilities.\n",
    "        \"\"\"\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select_action(state): Select an action from probability(policy) by sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(self, state):\n",
    "    \"\"\"\n",
    "    Selects an action based on the current policy.\n",
    "    \n",
    "    Args:\n",
    "    state (Tensor): The current state of the environment.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[int, Tensor, distributions.Categorical]:\n",
    "            - int: Index of the selected action.\n",
    "            - Tensor: Scaled continuous action.\n",
    "            - Categorical: Torch distribution object used for sampling/log_probs.\n",
    "    \"\"\"\n",
    "    probs = self.policy_net(state).to(self.device)\n",
    "    dist = distributions.Categorical(probs)\n",
    "    action_idx = dist.sample()\n",
    "    action = self.scale_action(action_idx.item())\n",
    "    return action_idx.item(), action, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate_stepwise_returns(rewards): Calculate return of each step in episode from reward list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stepwise_returns(self, rewards):\n",
    "    \"\"\"\n",
    "    Compute stepwise returns for the trajectory.\n",
    "\n",
    "    Args:\n",
    "        rewards (list(float)): List of rewards obtained in the episode.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Normalized stepwise returns.\n",
    "    \"\"\"\n",
    "    R = 0\n",
    "    returns = []\n",
    "    for r in reversed(rewards):\n",
    "        R = r + self.discount_factor * R\n",
    "        returns.insert(0, R)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32).to(self.device)\n",
    "    if len(returns) > 1:\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_trajectory(env): Run agent with current policy until end 1 episode and store log_probs and return for calculating loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_trajectory(self, env):\n",
    "    \"\"\"\n",
    "    Generate a trajectory by interacting with the environment. (can using with multi environments)\n",
    "\n",
    "    Args:\n",
    "        env: The environment object.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple(List[float], List[int], List[Tensor], List[Tensor], List[List[Tuple]]):\n",
    "        - List[float]: Total return for each environment.\n",
    "        - List[int]: Episode length for each environment.\n",
    "        - List[Tensor]: Discounted and normalized return for each step in each environment.\n",
    "        - List[Tensor]: Log probabilities of the actions taken at each step per environment.\n",
    "        - List[List[Tuple]]: Full trajectory (state, action, reward) per environment.\n",
    "        \n",
    "    \"\"\"\n",
    "    obs_list, _ = env.reset()\n",
    "    state_list = self.extract_policy_state(obs_list)\n",
    "    num_envs = len(state_list)\n",
    "    dones = [False] * num_envs\n",
    "    cumulative_rewards = [0.0] * num_envs\n",
    "    steps = [0] * num_envs\n",
    "    log_probs_list = [[] for _ in range(num_envs)]\n",
    "    rewards_list = [[] for _ in range(num_envs)]\n",
    "    trajectory_list = [[] for _ in range(num_envs)]\n",
    "    timestep = 0\n",
    "    while not all(dones):\n",
    "        actions_idx = []\n",
    "        actions = []\n",
    "        dists = []\n",
    "\n",
    "        for i, state in enumerate(state_list):\n",
    "            if dones[i]:\n",
    "                actions_idx.append(0)\n",
    "                actions.append(torch.tensor([[0.0]], dtype=torch.float32))\n",
    "                dists.append(None)\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                action_idx, action, dist = self.select_action(state_tensor)\n",
    "                actions.append(action)\n",
    "                actions_idx.append(action_idx)\n",
    "                dists.append(dist)\n",
    "        actions = torch.cat(actions, dim=0).to(self.device)\n",
    "\n",
    "        next_obs_list, rewards, terminations, truncations, _ = env.step(actions)\n",
    "        next_state_list = self.extract_policy_state(next_obs_list)\n",
    "\n",
    "        for i in range(num_envs):\n",
    "            if not dones[i]:\n",
    "                done = bool(terminations[i].item()) or bool(truncations[i].item())\n",
    "                log_probs_list[i].append(dists[i].log_prob(torch.tensor(actions_idx[i]).to(self.device)))\n",
    "                rewards_list[i].append(rewards[i].item())\n",
    "                trajectory_list[i].append((state_list[i], actions_idx[i], rewards[i].item()))\n",
    "                cumulative_rewards[i] += rewards[i].item()\n",
    "                steps[i] += 1\n",
    "                dones[i] = done\n",
    "                state_list[i] = next_state_list[i]\n",
    "\n",
    "        timestep += 1\n",
    "\n",
    "    all_returns = []\n",
    "    all_log_probs = []\n",
    "    for i in range(num_envs):\n",
    "        stepwise_returns = self.calculate_stepwise_returns(rewards_list[i])\n",
    "        all_returns.append(stepwise_returns)\n",
    "        all_log_probs.append(torch.stack(log_probs_list[i]).squeeze(-1))\n",
    "\n",
    "    return cumulative_rewards, steps, all_returns, all_log_probs, trajectory_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate_loss(returns_batch, log_probs_batch): using logprobs list of each step and return list of each step to calculate loss function of MC_REINFORCE network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(self, returns_batch, log_probs_batch):\n",
    "    \"\"\"\n",
    "    Compute the loss for policy optimization.\n",
    "\n",
    "    Args:\n",
    "        returns_batch (List[Tensor]): List of return tensors for each trajectory.\n",
    "        log_probs_batch (List[Tensor]): List of log-probability tensors for each trajectory.\n",
    "    \n",
    "    Returns:\n",
    "        Tensor: Computed loss.\n",
    "    \"\"\"\n",
    "    loss = torch.tensor(0.0, device=self.device)\n",
    "    for R, log_probs in zip(returns_batch, log_probs_batch):\n",
    "        loss += -(log_probs * R).sum()\n",
    "    loss /= sum(len(R) for R in returns_batch)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update_policy(returns_batch, log_probs_batch): Calculate loss function and update MC_REINFORCE network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(self, returns_batch, log_probs_batch):\n",
    "    \"\"\"\n",
    "    Update the policy using the calculated loss.\n",
    "\n",
    "    Args:\n",
    "        returns_batch (List[Tensor]): List of return tensors for each trajectory.\n",
    "        log_probs_batch (List[Tensor]): List of log-probability tensors for each trajectory.\n",
    "    \n",
    "    Returns:\n",
    "        float: Loss value after the update.\n",
    "    \"\"\"\n",
    "    loss = self.calculate_loss(returns_batch, log_probs_batch)\n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn(env): This is the main function that using in train code. In this function we train the agent for 1 episode. Statrt with reset enironment and generate trajectory that loop for playing action and store data then use that store data for update policy_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, env):\n",
    "    \"\"\"\n",
    "    Train the agent on a single episode. (can using with multi environments)\n",
    "\n",
    "    Args:\n",
    "        env: The environment to train in.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple(List[float], List[int], float, List[List[Tuple]]):\n",
    "            - List[float]: Total return per environment.\n",
    "            - List[int]: Episode length per environment.\n",
    "            - float: Policy loss after the update.\n",
    "            - List[List[Tuple]]: Trajectory of (state, action, reward) per env.\n",
    "    \"\"\"\n",
    "    self.policy_net.train()\n",
    "    episode_return, step, stepwise_returns, log_prob_actions, trajectory = self.generate_trajectory(env)\n",
    "    loss = self.update_policy(stepwise_returns, log_prob_actions)\n",
    "    return episode_return, step, loss, trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_model(path, filename), load_model(path, filename): save and load network model from input path and filename which is save as tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(self, path, filename):\n",
    "    \"\"\"\n",
    "    Save model network.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory to save model.\n",
    "        filename (str): Name of the file.\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    full_path = os.path.join(path, filename)\n",
    "    torch.save({\n",
    "        'policy_net': self.policy_net.state_dict(),\n",
    "        'optimizer': self.optimizer.state_dict(),\n",
    "    }, full_path)\n",
    "\n",
    "def load_model(self, path, filename):\n",
    "    \"\"\"\n",
    "    Load model network.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory to save model.\n",
    "        filename (str): Name of the file.\n",
    "    \"\"\"\n",
    "    full_path = os.path.join(path, filename)\n",
    "    checkpoint = torch.load(full_path, map_location=self.device)\n",
    "    self.policy_net.load_state_dict(checkpoint['policy_net'])\n",
    "    self.optimizer.load_state_dict(checkpoint['optimizer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C code is write following pseudo code in part1 and split to 7 function. And also have Actor_network class which is class for actor_net. and Critic_network class which is class for critic_net."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actor network: Neural network model for the A2C algorithm. for calculate probability of selecting each action (policy) for each state.\n",
    "- In Actor_network neural network is init as \n",
    "    - for discrete:Linear -> ReLU -> Linear -> ReLU -> Linear -> Softmax\n",
    "    - for continuous:\n",
    "        - for mu:Linear -> ReLU -> Linear -> ReLU -> Linear \n",
    "        - for std:Linear -> ReLU -> Linear -> ReLU -> Linear \n",
    "- Have function forward to pass data through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C_Actor(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim=1, is_discrete=True):\n",
    "        \n",
    "        super(A2C_Actor, self).__init__()\n",
    "        self.is_discrete = is_discrete\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        if self.is_discrete:\n",
    "            self.fc1 = nn.Linear(hidden_dim, output_dim)  # for logits → Categorical\n",
    "        else:\n",
    "            self.mu_net = nn.Linear(hidden_dim, 1)\n",
    "            self.std_net = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)  # Xavier initialization\n",
    "                nn.init.zeros_(m.bias)  # Initialize bias to 0\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = self.net(state)\n",
    "        if self.is_discrete:\n",
    "            logits = self.fc1(x)\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            return probs\n",
    "        else:\n",
    "            mu = self.mu_net(x)\n",
    "            log_std = self.std_net(x)\n",
    "            std = torch.exp(log_std.clamp(-20, 2))  # stability\n",
    "            return mu, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Critic network: Neural network model for the A2C algorithm. for calculate expected value of state.\n",
    "- In Critic_network neural network is init as Linear -> ReLU -> Linear -> ReLU -> Linear\n",
    "- Have function forward to pass data through the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class A2C_Critic(nn.Module):\n",
    "    def __init__(self, state_dim, hidden_dim):\n",
    "        \n",
    "        super(A2C_Critic, self).__init__()\n",
    "\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(state_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, 1)\n",
    "        )\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')  # Kaiming initialization\n",
    "                nn.init.zeros_(m.bias)  # Initialize bias to 0\n",
    "\n",
    "    def forward(self, state):\n",
    "        return self.net(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select_action(state): Selects an action based on the current policy from actor_net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(self, state):\n",
    "    with torch.no_grad():\n",
    "        if self.is_discrete:\n",
    "            probs = self.actor(state)\n",
    "            dist = distributions.Categorical(probs)\n",
    "            action_idx = dist.sample()\n",
    "            action = self.scale_action(action_idx.item())\n",
    "            return action_idx.item(), action\n",
    "        else:\n",
    "            mu, std = self.actor(state)\n",
    "            base_dist = distributions.Normal(mu, std)\n",
    "            dist = distributions.TransformedDistribution(base_dist, [distributions.TanhTransform(cache_size=1)])\n",
    "            action = dist.sample()\n",
    "            scaled_action = action * self.action_range[1]\n",
    "            return action, scaled_action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate_sample(batch_size): Generates a batch sample from memory(Replay Buffer) for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sample(self, batch_size):\n",
    "    if len(self.memory) < batch_size:\n",
    "        return None\n",
    "    states, actions, rewards, next_states, dones = self.memory.sample()\n",
    "    non_final_mask = ~dones\n",
    "    return non_final_mask, next_states, states, actions, rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate_loss(non_final_mask, next_states, state_batch, action_batch, reward_batch): Computes the loss for optimize actor_net and critic_net following equation in part1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(self, non_final_mask, next_states, state_batch, action_batch, reward_batch):\n",
    "    value = self.critic(state_batch).squeeze(-1)  # Estimate the value of the current state\n",
    "    next_values = self.critic(next_states).squeeze(-1).detach()\n",
    "    non_final_mask = non_final_mask.float()\n",
    "\n",
    "    # Target with full batch\n",
    "    target = reward_batch + non_final_mask * self.discount_factor * next_values\n",
    "    critic_loss = F.mse_loss(value, target)  # Calculate the critic loss\n",
    "\n",
    "    advantage = (target - value).detach()\n",
    "\n",
    "    if self.is_discrete:\n",
    "        probs = self.actor(state_batch)\n",
    "        dist = distributions.Categorical(probs)\n",
    "        log_probs = dist.log_prob(action_batch)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "    else:\n",
    "        mu, std = self.actor(state_batch)\n",
    "        base_dist = distributions.Normal(mu, std)\n",
    "        dist = distributions.TransformedDistribution(base_dist, [distributions.TanhTransform(cache_size=1)])\n",
    "        eps = 1e-6\n",
    "        action_batch = action_batch / self.action_range[1]  # scale to [-1, 1]\n",
    "        action_batch = action_batch.clamp(-1 + eps, 1 - eps)\n",
    "\n",
    "        log_probs = dist.log_prob(action_batch).sum(dim=-1)\n",
    "        entropy = base_dist.entropy().sum(dim=-1)\n",
    "\n",
    "    actor_loss = (-log_probs * advantage - self.entropy_coef * entropy).mean()\n",
    "\n",
    "    return actor_loss, critic_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update_policy(): sampling experience to calculate loss function and update actor and critic net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(self):\n",
    "    sample = self.generate_sample(self.batch_size)\n",
    "    if sample is None:\n",
    "        return 0.0, 0.0\n",
    "    non_final_mask, next_states, state_batch, action_batch, reward_batch = sample\n",
    "\n",
    "    reward_batch = (reward_batch - reward_batch.mean()) / (reward_batch.std() + 1e-7)\n",
    "\n",
    "    actor_loss, critic_loss = self.calculate_loss(non_final_mask, next_states, state_batch, action_batch, reward_batch)\n",
    "    \n",
    "    self.optimizer_critic.zero_grad()\n",
    "    critic_loss.backward()\n",
    "    self.optimizer_critic.step()\n",
    "\n",
    "    self.optimizer_actor.zero_grad()\n",
    "    actor_loss.backward()\n",
    "    self.optimizer_actor.step()\n",
    "\n",
    "\n",
    "    return actor_loss.item(), critic_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "learn(env): This is the main function that using in train code. In this function we train the agent for 1 episode. Statrt with reset enironment and loop to add experience to memory -> sampling -> calculate loss -> update actor and critic net until agent terminate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(self, env):\n",
    "    obs_list, _ = env.reset()\n",
    "    state_list = self.extract_policy_state(obs_list)\n",
    "    num_envs = len(state_list)\n",
    "    dones = [False] * num_envs\n",
    "    cumulative_rewards = [0.0] * num_envs\n",
    "    steps = [0] * num_envs\n",
    "    actor_losses = []\n",
    "    critic_losses = []\n",
    "\n",
    "    while not all(dones):\n",
    "        actions_idx = []\n",
    "        actions = []\n",
    "\n",
    "        for i, state in enumerate(state_list):\n",
    "            if dones[i]:\n",
    "                actions_idx.append(0)\n",
    "                actions.append(torch.tensor([[0.0]], dtype=torch.float32))\n",
    "            else:\n",
    "                state_tensor = torch.tensor(state, dtype=torch.float32).unsqueeze(0).to(self.device)\n",
    "                a_idx, a_cont = self.select_action(state_tensor)\n",
    "                actions_idx.append(a_idx)\n",
    "                actions.append(a_cont)\n",
    "        actions = torch.cat(actions, dim=0)\n",
    "\n",
    "        next_obs_list, rewards, terminations, truncations, _ = env.step(actions)\n",
    "        next_state_list = self.extract_policy_state(next_obs_list)\n",
    "\n",
    "        for i in range(num_envs):\n",
    "            if not dones[i]:\n",
    "                done = bool(terminations[i].item()) or bool(truncations[i].item())\n",
    "                self.memory.add(\n",
    "                    torch.tensor(state_list[i], dtype=torch.float32),\n",
    "                    actions_idx[i],\n",
    "                    rewards[i].item(),\n",
    "                    torch.tensor(next_state_list[i], dtype=torch.float32),\n",
    "                    done\n",
    "                )\n",
    "                cumulative_rewards[i] += rewards[i].item()\n",
    "                steps[i] += 1\n",
    "                dones[i] = done\n",
    "                state_list[i] = next_state_list[i]\n",
    "\n",
    "        actor_loss, critic_loss = self.update_policy()\n",
    "        actor_losses.append(actor_loss)\n",
    "        critic_losses.append(critic_loss)\n",
    "\n",
    "    return cumulative_rewards, steps, np.mean(actor_losses), np.mean(critic_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save_model(path, filename), load_model(path, filename): save and load network model from input path and filename which is save as tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(self, path, filename):\n",
    "    \"\"\"\n",
    "    Save model network.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory to save model.\n",
    "        filename (str): Name of the file.\n",
    "    \"\"\"\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    full_path = os.path.join(path, filename)\n",
    "    torch.save({\n",
    "        'actor': self.actor.state_dict(),\n",
    "        'critic': self.critic.state_dict(),\n",
    "        'optimizer_actor': self.optimizer_actor.state_dict(),\n",
    "        'optimizer_critic': self.optimizer_critic.state_dict(),\n",
    "    }, full_path)\n",
    "\n",
    "def load_model(self, path, filename):\n",
    "    \"\"\"\n",
    "    Load model network.\n",
    "\n",
    "    Args:\n",
    "        path (str): Directory to save model.\n",
    "        filename (str): Name of the file.\n",
    "    \"\"\"\n",
    "    full_path = os.path.join(path, filename)\n",
    "    checkpoint = torch.load(full_path, map_location=self.device)\n",
    "    self.actor.load_state_dict(checkpoint['actor'])\n",
    "    self.critic.load_state_dict(checkpoint['critic'])\n",
    "    self.optimizer_actor.load_state_dict(checkpoint['optimizer_actor'])\n",
    "    self.optimizer_critic.load_state_dict(checkpoint['optimizer_critic'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Trainning & Playing to stabilize Cart-Pole Agent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement the training loop collect data, analyze results, and save models for evaluating agent performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In each algorithm we put training loop of each step in learn(env) function so in training loop in train code just loop learn for episode time. And log data in wandb (log data have different in each model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset environment\n",
    "timestep = 0\n",
    "sum_reward = 0\n",
    "# simulate environment\n",
    "while simulation_app.is_running():\n",
    "    # run everything in inference mode\n",
    "    # with torch.inference_mode():\n",
    "    \n",
    "    for episode in tqdm(range(n_episodes)):\n",
    "        cumulative_rewards, steps, losses = agent.learn(env)\n",
    "\n",
    "        cumulative_reward = sum(cumulative_rewards) / len(cumulative_rewards)\n",
    "        step = sum(steps) / len(steps)\n",
    "        loss = sum(losses) / len(losses)\n",
    "\n",
    "        moving_avg_window.append(cumulative_reward)\n",
    "        moving_avg_reward = sum(moving_avg_window) / len(moving_avg_window)\n",
    "\n",
    "        moving_avg_window2.append(step)\n",
    "        moving_avg_step = sum(moving_avg_window2) / len(moving_avg_window2)\n",
    "\n",
    "        moving_avg_window3.append(loss)\n",
    "        moving_avg_loss = sum(moving_avg_window3) / len(moving_avg_window3)\n",
    "        \n",
    "        wandb.log({\n",
    "            \"avg_reward\" : moving_avg_reward,\n",
    "            \"reward\" : cumulative_reward,\n",
    "            \"avg_step\" : moving_avg_step,\n",
    "            \"step\" : step,\n",
    "            \"avg_loss\" : moving_avg_loss,\n",
    "            \"loss\" : loss,\n",
    "            \"epsilon\" : agent.epsilon\n",
    "        })\n",
    "\n",
    "        sum_reward += cumulative_reward\n",
    "        if episode % 100 == 0:\n",
    "            print(\"avg_score: \", sum_reward / 100.0)\n",
    "            sum_reward = 0\n",
    "            print(agent.epsilon)\n",
    "\n",
    "            # Save Q-Learning agent\n",
    "            w_file = f\"{Algorithm_name}_{episode}_{num_of_action}_{action_range[1]}.json\"\n",
    "            full_path = os.path.join(f\"model/{task_name}\", f\"{Algorithm_name}/{exp_name}\")\n",
    "            agent.save_model(full_path, w_file)\n",
    "    \n",
    "    print('Complete')\n",
    "    # agent.plot_durations(show_result=True)\n",
    "    plt.ioff()\n",
    "    plt.show()\n",
    "        \n",
    "    if args_cli.video:\n",
    "        timestep += 1\n",
    "        # Exit the play loop after recording one video\n",
    "        if timestep == args_cli.video_length:\n",
    "            break\n",
    "\n",
    "    break\n",
    "# ==================================================================== #\n",
    "\n",
    "# close the simulator\n",
    "wandb.finish()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log data\n",
    "- reward\n",
    "- step (episode lenght)\n",
    "- loss\n",
    "- epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter\n",
    "- num_of_action\n",
    "- action_range\n",
    "- learning_rate\n",
    "- n_episodes\n",
    "- initial_epsilon\n",
    "- epsilon_decay\n",
    "- final_epsilon\n",
    "- discount_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear Q-lerning experiment: I have choose important parameter for tuning which is learning_rate, epsilon_decay, discount_factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 1: changing learning rate\n",
    "Learning rate is using for controls how much new weight overrides old weight.\n",
    "\n",
    "In this experiment have change learning rate to 2 value [0.0001, 0.001]\n",
    "\n",
    "Hypothesis: Changing learning rate higher make faster learning but higher variance.\n",
    "\n",
    "Reward, step, loss, epsilon graph:\n",
    "![Linear_lr.png](img/Linear_lr.png)\n",
    "\n",
    "conclude: \n",
    "\n",
    "A higher learning rate (0.001) was expected to speed up learning, but in practice, it caused unstable updates that pushed the weights away from optimal values. As a result, the agent failed to solve the task.\n",
    "\n",
    "In contrast, a lower learning rate (0.0001) led to more stable learning and higher rewards, confirming that learning rate 0.0001 (gradual updates) were better suited for the Linear Q-Learning model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 2: changing discount factor\n",
    "Discount factor determines how much the agent values future rewards compared to immediate ones.\n",
    "\n",
    "In this experiment have change discount factor to 2 value [0.95, 0.99]\n",
    "\n",
    "Hypothesis: A higher discount factor (0.99) encourages long-term reward planning, leading to potentially higher returns, but may cause greater variance in learning\n",
    "\n",
    "Reward, step, loss, epsilon graph:\n",
    "![Linear_d.png](img/Linear_d.png)\n",
    "\n",
    "conclude: \n",
    "A higher discount factor (0.99) leads to slightly more variance, but similar peak performance compared to 0.95. It promotes longer-term planning but does not significantly outperform in this setup."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 3: changing epsilon decay\n",
    "Epsilon decay is controls how fast the agent shifts from exploration to exploitation.\n",
    "\n",
    "In this experiment have change epsilon decay to 2 value [0.9995, 0.9993]\n",
    "\n",
    "Hypothesis: A higher epsilon decay (0.9995) results in more exploration, leading to slower learning but more accurate and stable performance in the long term.\n",
    "\n",
    "Reward, step, loss, epsilon graph:\n",
    "![Linear_epsilon.png](img/Linear_epsilon.png)\n",
    "\n",
    "conclude: \n",
    "\n",
    "A lower epsilon decay (0.9993) caused the agent to exploit too early, resulting in limited exploration and poor performance.\n",
    "\n",
    "In contrast, higher decay (0.9995) allowed for more exploration, helping the agent reach higher rewards and better long-term performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log data\n",
    "- reward\n",
    "- step (episode lenght)\n",
    "- loss\n",
    "- epsilon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter\n",
    "- num_of_action\n",
    "- action_range\n",
    "- learning_rate\n",
    "- n_episodes\n",
    "- initial_epsilon\n",
    "- epsilon_decay\n",
    "- final_epsilon\n",
    "- discount_factor\n",
    "- hidden_dim\n",
    "- buffer_size\n",
    "- batch_size\n",
    "- dropout\n",
    "- tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the DQN experiment, the following hyperparameters were selected for tuning: hidden_dim, buffer_size, batch_size, dropout, tau, and learning_rate. These parameters directly affect the capacity, stability, and learning dynamics of the neural network and replay mechanism in DQN.\n",
    "\n",
    "We did not vary discount_factor or epsilon decay in this experiment, as their impact has already been observed and analyzed in the Linear Q-Learning experiments. Since DQN also uses an epsilon-greedy policy and discounted returns in a similar way, we expect them to behave similarly in DQN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 1: changing hidden_dim\n",
    "Hidden dim is controls capacity of the neural network. A larger hidden dimension allows the model to represent more complex Q-functions.\n",
    "\n",
    "In this experiment have change hidden_dim to 3 value [64, 128, 256]\n",
    "\n",
    "**Hypothesis:** Increasing hidden_dim improves the model's ability to approximate Q-values, leading to better learning performance.\n",
    "\n",
    "**Reward, step, loss, epsilon graph:**\n",
    "![DQN_hidden_dim.png](img/DQN_hidden_dim.png)\n",
    "\n",
    "**conclude:** \n",
    "\n",
    "As hypothesized, higher hidden dimensions (128 and 256) performed better than 64, showing higher rewards and longer episodes.\n",
    "However, the difference between 128 and 256 is minor, suggesting that 128 may be enough for this task, and increasing model size may make overfitting and higher variance in action selection due to more complex Q-value estimation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 2: changing buffer_size\n",
    "Buffer size is controls size of memory that stores past experiences.\n",
    "\n",
    "In this experiment have change buffer_size to 2 value [1000, 5000]\n",
    "\n",
    "**Hypothesis:** Increasing buffer_size make agent have more choice when sampling and make agent n\n",
    "ot over-fittng in recent samples.\n",
    "\n",
    "**Reward, step, loss, epsilon graph:**\n",
    "![DQN_buffer.png](img/DQN_buffer.png)\n",
    "\n",
    "**conclude:** \n",
    "\n",
    "From the graph, we observe that buffer size 5000 introduces more variance in training performance.\n",
    "This may be caused by the agent learning from older, less relevant experiences, which increases the variance in sampled batches.\n",
    "\n",
    "Although larger buffer sizes improve sample diversity, they can also reduce training stability if too many outdated transitions are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 3: changing batch_size\n",
    "Batch size is determines how many samples are used in each training update.\n",
    "\n",
    "In this experiment have change batch_size to 3 value [64, 128, 256]\n",
    "\n",
    "**Hypothesis:** Larger batch sizes provide more stable gradient estimates, which should reduce update variance and lead to smoother learning.\n",
    "\n",
    "**Reward, step, loss, epsilon graph:**\n",
    "![DQN_batch.png](img/DQN_batch.png)\n",
    "\n",
    "**conclude:**\n",
    "From the graph, we observe that batch size does not significantly impact performance in this setup.\n",
    "All three values (64, 128, 256) lead to similar rewards, steps, and loss trends.\n",
    "This suggests that within this range, the DQN agent is robust to changes in batch size, and no clear advantage is observed for larger batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 4: changing dropout\n",
    "Dropout is a regularization technique used to prevent overfitting by randomly deactivating neurons during training.\n",
    "\n",
    "In this experiment have change dropout to 2 value [0.4, 0.5]\n",
    "\n",
    "**Hypothesis:** A lower dropout rate keeps more neurons active, which may speed up learning but risks overfitting.\n",
    "\n",
    "**Reward, step, loss, epsilon graph:**\n",
    "![DQN_dropout.png](img/DQN_dropout.png)\n",
    "\n",
    "**conclude:**\n",
    "From the graph, the lower dropout (0.4) performs slightly better in terms of reward and step count.\n",
    "This is likely because more neurons remain active, allowing the network to learn more effectively.\n",
    "\n",
    "However, very low dropout values could lead to overfitting, especially in more complex tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 5: changing tau\n",
    "Tau is use for controls how fast the target network follows the policy network.\n",
    "\n",
    "In this experiment have change tau to 2 value [0.01, 0.005]\n",
    "\n",
    "**Hypothesis:** Higher tau updates the target network more aggressively, which may introduce bias and unstable reward.\n",
    "\n",
    "**Reward, step, loss, epsilon graph:**\n",
    "![DQN_tau.png](img/DQN_tau.png)\n",
    "\n",
    "**conclude:**\n",
    "The graph shows that higher tau (0.01) results in more unstable rewards in later episodes.\n",
    "This supports the hypothesis that faster target updates introduce more bias, making learning less stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 6: changing learning rate\n",
    "Learning rate is using for controls how much new model override existing weights during optimization.\n",
    "\n",
    "In this experiment have change learning rate to 2 value [0.0001, 0.001]\n",
    "\n",
    "**Hypothesis:** Changing learning rate higher make faster learning but higher variance.\n",
    "\n",
    "**Reward, step, loss, epsilon graph:**\n",
    "![DQN_lr.png](img/DQN_lr.png)\n",
    "\n",
    "**conclude:**\n",
    "A higher learning rate (0.001) leads to faster learning early on, but causes more aggressive and unstable updates later.\n",
    "This results in the agent being less consistent and unable to reach higher reward levels, compared to the more stable and reliable performance of the lower learning rate (0.0001)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MC REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log data\n",
    "- reward\n",
    "- step (episode lenght)\n",
    "- loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter\n",
    "- num_of_action\n",
    "- action_range\n",
    "- learning_rate\n",
    "- n_episodes\n",
    "- hidden_dim\n",
    "- discount_factor\n",
    "- dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the MC REINFORCE experiment, the following hyperparameters were selected for tuning: learning rate, discount factor, dropout.\n",
    "\n",
    "We did not vary hidden_dim in this experiment, as their impact has already been observed and analyzed in the DQN experiments. Since MC REINFORCE uses a similar neural network structure to DQN, we assume the effect of hidden_dim would be consistent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 1: changing learning rate\n",
    "Learning rate is using for controls how much new model override existing weights during optimization.\n",
    "\n",
    "In this experiment have change learning rate to 2 value [0.0001, 0.001]\n",
    "\n",
    "**Hypothesis:** Changing learning rate higher make faster learning but higher variance.\n",
    "\n",
    "**Reward, step, loss, epsilon graph:**\n",
    "![MC_lr.png](img/MC_lr.png)\n",
    "\n",
    "**conclude:**\n",
    "\n",
    "A higher learning rate (0.001) led to faster and stronger learning, with the agent reaching significantly higher average rewards compared to 0.0001.\n",
    "However, it also introduced more variance in training.\n",
    "\n",
    "Compared to DQN, MC REINFORCE appears more sensitive to learning rate—the lower value (0.0001) resulted in very slow learning and poor performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 2: changing discount factor\n",
    "Discount factor determines how much the agent values future rewards compared to immediate ones.\n",
    "\n",
    "In this experiment have change discount factor to 2 value [0.95, 0.99]\n",
    "\n",
    "Hypothesis: A higher discount factor (0.99) encourages long-term reward planning, leading to potentially higher returns, but may cause greater variance in learning\n",
    "\n",
    "Reward, step, loss, epsilon graph:\n",
    "![MC_d.png](img/MC_d.png)\n",
    "\n",
    "conclude: \n",
    "\n",
    "Initially, both discount factors performed similarly. However, over time, the higher discount factor (0.99) achieved better long-term rewards.However, it also made the training less stable, with ups and downs and some big drops in reward.\n",
    "\n",
    "This matches the hypothesis that a higher discount factor helps the agent plan for the future, but can also make learning more unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 3: changing dropout\n",
    "Dropout is a regularization technique used to prevent overfitting by randomly deactivating neurons during training.\n",
    "\n",
    "In this experiment have change dropout to 2 value [0.4, 0.5]\n",
    "\n",
    "**Hypothesis:** A lower dropout rate keeps more neurons active, which may speed up learning but risks overfitting.\n",
    "\n",
    "**Reward, step, loss, epsilon graph:**\n",
    "![MC_dropout.png](img/MC_dropout.png)\n",
    "\n",
    "**conclude:**\n",
    "The graph shows that changing dropout between 0.4 and 0.5 had minimal effect on learning performance.\n",
    "This suggests that MC REINFORCE is robust to minor dropout changes, or that the dropout-affected neurons may not have played a significant role in decision-making."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A2C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "log data\n",
    "- reward\n",
    "- step (episode lenght)\n",
    "- actor loss\n",
    "- critic loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hyper parameter\n",
    "- num_of_action\n",
    "- action_range\n",
    "- learning_rate\n",
    "- hidden_dim\n",
    "- n_episodes\n",
    "- discount_factor\n",
    "- batch_size\n",
    "- buffer_size\n",
    "- entropy_coef"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the A2C experiment, the following hyperparameters were selected for tuning: learning rate, discount factor, entropy_coef, hidden_dim."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 1: changing learning rate\n",
    "Learning rate is using for controls how much new model override existing weights during optimization.\n",
    "\n",
    "In this experiment have change learning rate to 2 value [0.00001, 0.00005]\n",
    "\n",
    "**Hypothesis:** Changing learning rate higher make faster learning but higher variance.\n",
    "\n",
    "**Reward, step, loss, epsilon graph:**\n",
    "![A2C_lr.png](img/A2C_lr.png)\n",
    "\n",
    "**conclude:**\n",
    "A higher learning rate (0.00005) helped the agent learn faster.\n",
    "However, there was no big difference in stability between the two values.\n",
    "This shows that A2C can learn well with both low and high learning rates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 2: changing discount factor\n",
    "Discount factor determines how much the agent values future rewards compared to immediate ones.\n",
    "\n",
    "In this experiment have change discount factor to 2 value [0.95, 0.99]\n",
    "\n",
    "Hypothesis: A higher discount factor (0.99) encourages long-term reward planning, leading to potentially higher returns, but may cause greater variance in learning\n",
    "\n",
    "Reward, step, loss, epsilon graph:\n",
    "![A2C_d.png](img/A2C_d.png)\n",
    "\n",
    "conclude: \n",
    "The graph shows almost no difference between 0.95 and 0.99.\n",
    "This means A2C is robust to small changes in the discount factor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 3: changing entropy_coef\n",
    "Entropy coeficient is use for encourages exploration by penalizing overconfidence in policy.\n",
    "\n",
    "In this experiment have change hidden_dim to 2 value [64, 128]\n",
    "\n",
    "**Hypothesis:** Higher entropy means more exploration, which may slow down learning but help avoid bad policies.\n",
    "\n",
    "**Reward, step, loss, epsilon graph:**\n",
    "![A2C_eny.png](img/A2C_en.png)\n",
    "\n",
    "**conclude:** \n",
    "Higher entropy (0.05) made the agent explore more, but slowed down learning.\n",
    "Lower entropy (0.01) helped the agent learn faster and get higher rewards.\n",
    "So, too much exploration may hurt performance once the agent is already doing well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### experiment 4: changing hidden_dim\n",
    "Hidden dim is controls capacity of the neural network. A larger hidden dimension allows the model to represent more complex Q-functions and policy-function.\n",
    "\n",
    "In this experiment have change hidden_dim to 2 value [64, 128]\n",
    "\n",
    "**Hypothesis:** Increasing hidden_dim improves the model's ability to approximate Q-values and policy, leading to better learning performance.\n",
    "\n",
    "**Reward, step, loss, epsilon graph:**\n",
    "![A2C_hidden_dim.png](img/A2C_hidden_dim.png)\n",
    "\n",
    "**conclude:** \n",
    "As hypothesis, the larger hidden size (128) helped the agent learn faster and reach slightly better rewards.\n",
    "Bigger networks can help, but the difference wasn't huge in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Evaluate Cart-Pole Agent performance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
