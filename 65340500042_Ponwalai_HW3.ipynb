{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 Function-based RL\n",
    "Ponwalai Chalermwattanatrai 65340500042"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each algorithm, describe whether it follows a value-based, policy-based, or Actor-Critic approach, specify the type of policy it learns (stochastic or deterministic), identify the type of observation space and action space (discrete or continuous), and explain how each advanced RL method balances exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Linear Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Approach**: Value-based\n",
    "- **Policy type**: Deterministic (Stochastic during training)\n",
    "    - Always chooses the action with the maximum Q-value (argmax Q(s, a)) -> Deterministic\n",
    "    - During training, using epsilon-greedy, which includes some random -> Stochastic\n",
    "- **Observation space**: continuous\n",
    "- **Action space**: discrete\n",
    "- **Balances exploration and exploitation**: using epsilon-greedy\n",
    "    - Selects a random action with probability epsilon and decay epsilon over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept of algorithm**\n",
    "\n",
    "Linear Q-learning is a Q-learning algorithm that use function approximation instead of a Q-table. It stores a weight matrix, where each column corresponds to the weights for a particular action. The size of the weight matrix is [state_size x num_actions].\n",
    "\n",
    "In Linear- Q learning Q-value is linear function calculated using the dot product between the state vector and the weight vector for the selected action:\n",
    "\n",
    "$$Q(s, a) = \\mathbf{w}_a^\\top \\mathbf{s}$$\n",
    "\n",
    "After get action and apply to environment, to update the weights, we calculate the Temporal Difference (TD) error using the Bellman equation:\n",
    "\n",
    "$$\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$$\n",
    "\n",
    "and define loss function as the mean squared error of TD error (1/2 is Constant that add for easier differential equation which is not impact to value):\n",
    "\n",
    "$$\\mathcal{L} = \\frac{1}{2} \\delta^2$$\n",
    "\n",
    "To minimize this loss, we apply gradient descent. The gradient of the loss with respect to the weights is:\n",
    "\n",
    "$$\\nabla_{\\mathbf{w}_a} \\mathcal{L} = -\\delta \\cdot \\mathbf{s}$$\n",
    "\n",
    "Therefore, the weights are updated using the following rule:\n",
    "\n",
    "$$\\mathbf{w}_a \\leftarrow \\mathbf{w}_a + \\alpha \\cdot \\delta \\cdot \\mathbf{s}$$\n",
    "\n",
    "Which can be written as:\n",
    "\n",
    "$$\\boxed{\n",
    "\\mathbf{w}_a^{\\text{new}} = \\mathbf{w}_a^{\\text{old}} + \\alpha \\left( r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right) \\cdot \\mathbf{s}\n",
    "}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Psuedo code of training Linear Q learning:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Initialize weight vector w[a] for each action a\n",
    "Set epsilon to initial value\n",
    "\n",
    "For each episode:\n",
    "    Initialize environment and get initial state\n",
    "    state = initial_state\n",
    "    done = false\n",
    "\n",
    "    While not done:\n",
    "        # epsilon-greedy action selection\n",
    "        Generate a random number r ∈ [0, 1]\n",
    "        if r < epsilon:\n",
    "            action = random action\n",
    "        else:\n",
    "            for each action a:\n",
    "                Q[a] = dot(state, w[a])\n",
    "            action = action with max Q[a]\n",
    "\n",
    "        Take action in the environment\n",
    "        Observe next_state, reward, done\n",
    "\n",
    "        # TD Update\n",
    "        current_Q = dot(state, w[action])\n",
    "        For each action a:\n",
    "            next_Q[a] = dot(next_state, w[a])\n",
    "        max_next_Q = max over next_Q[a]\n",
    "\n",
    "        td_target = reward + discount_factor * max_next_Q\n",
    "        td_error = td_target - current_Q\n",
    "\n",
    "        # Gradient descent update\n",
    "        w[action] = w[action] + learning_rate * td_error * state\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    Decay epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. DQN (Deep Q-Learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Approach**: Value-based\n",
    "- **Policy type**: Deterministic (Stochastic during training)\n",
    "    - Always chooses the action with the maximum Q-value (argmax Q(s, a)) -> Deterministic\n",
    "    - During training, using epsilon-greedy, which includes some random -> Stochastic\n",
    "- **Observation space**: continuous\n",
    "- **Action space**: discrete\n",
    "- **Balances exploration and exploitation**: using epsilon-greedy\n",
    "    - Selects a random action with probability epsilon and decay epsilon over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept of algorithm**\n",
    "\n",
    "DQN (Deep Q-Network) is conceptually similar to Linear Q-learning, but instead of using a linear function for function approximation, DQN uses a neural network to approximate the Q-value function.\n",
    "\n",
    "Structure of DQN\n",
    "\n",
    "![image2.png](img/image2.png)\n",
    "\n",
    "source: https://medium.com/@samina.amin/deep-q-learning-dqn-71c109586bae\n",
    "\n",
    "DQN has 3 main components:\n",
    "1. replay memory\n",
    "    - A buffer used to store the agent’s past experiences as tuples:(state, action, reward, next state)\n",
    "    - Using for traning, during training it randomly sample mini-batches of experiences from the buffer to update policy net.\n",
    "    - mini-batches will breaks the temporal correlation between consecutive experiences and improves stability and sample efficiency.\n",
    "\n",
    "2. Policy net\n",
    "    - A neural network that approximates the Q-value.\n",
    "    - Similar to the weight vector in Linear Q-learning but with hidden layers for greater function approximation.\n",
    "    - The network is updated using TD error and gradient descent:\n",
    "    $$\\delta = r + \\gamma \\max_{a'} Q(s', a') - Q(s, a)$$\n",
    "    - The loss function is the smooth L1 loss (Huber loss):\n",
    "    $$\n",
    "    \\mathcal{L} =\n",
    "    \\begin{cases}\n",
    "    \\frac{1}{2} \\delta^2 & \\text{if } |\\delta| < 1 \\\\\n",
    "    |\\delta| - \\frac{1}{2} & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "    $$\n",
    "    - This behaves like MSE for small TD errors and like MAE for large errors — making it less sensitive to outliers.\n",
    "\n",
    "3. Target net\n",
    "    - A copy of the policy network that is used to calculate the Q-value for the next state.\n",
    "    - It prevents the TD target from shifting too quickly, which can destabilize training.\n",
    "    - Rather than updating the target network every step, DQN uses a soft update mechanism.\n",
    "    $$\\theta_{\\text{target}} \\leftarrow \\tau \\cdot \\theta_{\\text{policy}} + (1 - \\tau) \\cdot \\theta_{\\text{target}}$$\n",
    "    Where:\n",
    "    - $\\theta_{\\text{policy}}$​: weights of the policy network\n",
    "    - $\\theta_{\\text{target}}$​: weights of the target network\n",
    "    - $\\tau$: soft update rate (between [0,1])\n",
    "    \n",
    "    Soft update is prevents rapid oscillations in learning\n",
    "    \n",
    "**The relationship of 3 component is**\n",
    "\n",
    "![image3.png](img/image3.png)\n",
    "\n",
    "source: https://www.theengineeringprojects.com/2024/01/deep-q-networks-dqn-reinforcement-learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Psuedo code of training DQN:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Initialize replay_memory with size = buffer_size\n",
    "    - Stores (state, action, reward, next_state, done) tuples\n",
    "    - Follows FIFO replacement when full\n",
    "\n",
    "Initialize policy_net: a neural network\n",
    "    - Input size = state dimension\n",
    "    - Output size = number of actions\n",
    "\n",
    "Initialize target_net: a copy of policy_net\n",
    "    - Same architecture as policy_net\n",
    "    - Used to compute stable TD targets\n",
    "\n",
    "Set epsilon to initial value\n",
    "\n",
    "for each episode:\n",
    "    Initialize environment and get initial state\n",
    "    state = initial_state\n",
    "    done = false\n",
    "\n",
    "    While not done:\n",
    "        # epsilon-greedy action selection\n",
    "        Generate a random number r ∈ [0, 1]\n",
    "        if r < epsilon:\n",
    "            action = random action\n",
    "        else:\n",
    "            Q_values = policy_net(state) # Q_values size = number of actions\n",
    "            action = argmax(Q_values)\n",
    "\n",
    "        Take action in the environment\n",
    "        Observe next_state, reward, done\n",
    "        Store (state, action, reward, next_state, done) in replay_memory\n",
    "\n",
    "        # Sample mini-batch from replay_memory\n",
    "        batch = sample(batch_size) from replay_memory\n",
    "        Initialize loss_list = []\n",
    "\n",
    "        #update policy_net\n",
    "        for (s, a, r, s', is_done) in batch:\n",
    "            current_Q = policy_net(s)[a]\n",
    "            if is_done:\n",
    "                td_target = r\n",
    "            else:\n",
    "                next_Q = max(target_net(s'))\n",
    "                td_target = r + discount_factor * next_Q\n",
    "            loss = smooth_l1_loss(current_Q, td_target)\n",
    "            add loss to loss_list\n",
    "        mean_loss = average(loss_list)\n",
    "        Perform gradient descent on policy_net using mean_loss\n",
    "\n",
    "        #Soft update target_net\n",
    "        for each parameter key in policy_net:\n",
    "            target_net[key] = tau * policy_net[key] + (1 - tau) * target_net[key]\n",
    "\n",
    "        state = next_state\n",
    "    \n",
    "    Decay epsilon\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. MC REINFORCE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Approach**: Policy-based\n",
    "- **Policy type**: Stochastic\n",
    "    - Learns a probability distribution over actions (Categorical distribution)\n",
    "- **Observation space**: continuous\n",
    "- **Action space**: discrete\n",
    "- **Balances exploration and exploitation**: using stochastic policy\n",
    "    - In MC REINFORCE action sample from probability distribution, this means the agent naturally explores, since each action has some non-zero probability of being selected.\n",
    "    - Exploit start when policy increases the probability of high-reward actions through policy gradient updates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Concept of algorithm**\n",
    "\n",
    "MC REINFORCE is a Monte Carlo method which using policy gradient to approximate policy. It using complete return of the entire episode from current policy to update new policy.\n",
    "MC REINFORCE is a Monte Carlo policy gradient method that learns a stochastic policy by using complete returns from sampled episodes. It updates the policy by increasing the probability of actions that lead to high rewards.\n",
    "\n",
    "Action Selection:\n",
    "- MC REINFORCE is policy-based, meaning it directly models the policy $\\pi(a∣s)$ as a neural network (policy net).\n",
    "- Given a state s, the policy net outputs a probability distribution over actions.\n",
    "- A Categorical distribution is constructed from these probabilities, and an action is sampled from it\n",
    "\n",
    "Policy Update\n",
    "- In policy update we want to maximize the expected return:\n",
    "$$J(\\theta) = \\mathbb{E} [ \\sum_{t=0}^{T} r_t ]$$\n",
    "- Using the episodic policy gradient theorem, we get a formula for how to change the policy parameters $\\theta$ to increase $J(\\theta)$:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E} [\\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot G_t ]$$\n",
    "- So we need to minimize $-\\nabla_\\theta J(\\theta)$, so we give loss function = $-\\nabla_\\theta J(\\theta)$:\n",
    "$$\\mathcal{L} = -\\nabla_\\theta J(\\theta) = - \\sum_{t=0}^{T} \\log \\pi_\\theta(a_t \\mid s_t) \\cdot G_t$$\n",
    "- when we minimize loss function is equal to maximize policy gradient\n",
    "\n",
    "from equation, to update the policy, we need:\n",
    "1. Returns $G_t$​ at each timestep:\n",
    "    - Calculate from the total discounted future rewards from time t: \n",
    "    $$G_t = r_t + \\gamma r_{t+1} + \\gamma^2 r_{t+2} + ... = r_t + \\gamma G_{t+1}​$$\n",
    "2. Log-probabilities of selected actions:\n",
    "    - For each timestep, we store:\n",
    "    $$log\\pi(a_t​∣s_t​)$$\n",
    "    - This term gives us a score function gradient, which tells us how the log-probability of the action changes with respect to the policy parameters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Psuedo code of training MC REINFORCE:**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "Initialize policy_net: a neural network\n",
    "    - Input size = state dimension\n",
    "    - Output size = number of actions (via softmax)\n",
    "\n",
    "for each episode:\n",
    "    Initialize environment and get initial state\n",
    "    state = initial_state\n",
    "    done = false\n",
    "\n",
    "    Initialize reward_list = []\n",
    "    Initialize log_probs_list = []\n",
    "\n",
    "    while not done:\n",
    "        # select action from policy\n",
    "        probability = policy_net(state)\n",
    "        distribution = Categorical(probability)\n",
    "        action = sample from distribution\n",
    "\n",
    "        Take action in the environment\n",
    "        Observe next_state, reward, done\n",
    "        add reward to reward_list\n",
    "        add log(distribution[action]) to log_probs_list\n",
    "\n",
    "        state = next_state\n",
    "\n",
    "    #calculate stepwise returns\n",
    "    Initialize return_list = []\n",
    "    for r in reverse(reward):\n",
    "        G = r + discount_factor * G\n",
    "        add G to return_list\n",
    "\n",
    "    #update policy\n",
    "    loss = 0\n",
    "    for each (r, log_prob) in (return_list, log_probs_list)\n",
    "        loss = loss - (log_prob * r)\n",
    "    loss = loss / len(return_list)\n",
    "    \n",
    "    Perform gradient descent on policy_net using loss"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
