{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3 Function-based RL\n",
    "Ponwalai Chalermwattanatrai 65340500042"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each algorithm, describe whether it follows a value-based, policy-based, or Actor-Critic approach, specify the type of policy it learns (stochastic or deterministic), identify the type of observation space and action space (discrete or continuous), and explain how each advanced RL method balances exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Linear Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Approach**: Value-based\n",
    "- **Policy type**: Deterministic (Stochastic during training)\n",
    "    - Always chooses the action with the maximum Q-value (argmax Q(s, a)) -> Deterministic\n",
    "    - During training, using epsilon-greedy, which includes some random -> Stochastic\n",
    "- **Observation space**: continuous\n",
    "- **Action space**: discrete\n",
    "- **Balances exploration and exploitation**: using epsilon-greedy\n",
    "    - Selects a random action with probability epsilon and decay epsilon over time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isaaclab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
